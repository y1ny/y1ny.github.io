{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI for Psycholinguisitcs\n",
    "主要内容：使用自然语言处理模型提取语言特征和进行语言任务"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 加载工具包\n",
    "\n",
    "* 使用到的自然语言处理工具包\n",
    "    - **srilm**: 计算频次、转移概率。[[官方文档]](https://srilm-python.readthedocs.io/en/latest/#)\n",
    "    - **hanlp**: 适用于基本自然语言处理任务，包括分词、词性分析、句法分析、语义分析、静态词向量提取等等，对于中文比较友好。[[官方文档]](https://hanlp.hankcs.com/docs/)\n",
    "    - **Huggingface系列**: 调用开源深度学习模型完成以上两者提到的，以及更多其他的任务，包括情感分析、文本生成等等。[[官网]](https://huggingface.co/)\n",
    "* 其他常用的自然语言处理工具包\n",
    "    - **nltk**：可以调用众多语料库（如wordnet等），也可以进行一系列的自然语言处理任务。[[官方文档]](https://www.nltk.org/)\n",
    "    - **spacy**：速度快、功能全面的自然语言处理工具包。[[官方文档]](https://spacy.io/)\n",
    "    - **stanza**：Stanford CoreNLP的python版本\n",
    "    - **fastNLP**：复旦大学制作的NLP工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果在colab等服务器上运行，先用以下命令去掉#安装工具包\n",
    "#!pip install srilm\n",
    "#!pip install hanlp\n",
    "#!pip install transformers, tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果只需要提取一部分特征，可以选择性地导入以下工具包\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 数据处理及可视化\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.rc(\"font\", family='SimHei') # 用来显示中文，对于macos系统需要换一个支持的字体\n",
    "\n",
    "# 自然语言处理\n",
    "from srilm import LM\n",
    "import hanlp\n",
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    GPT2LMHeadModel, \n",
    "    TextGenerationPipeline,\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 词汇及语义特征提取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 数据预处理：加载语料库以及进行分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_str(astr, tokenizer):\n",
    "    '''\n",
    "    # 使用分词模型来分词\n",
    "    输入: \n",
    "        astr: str, a sentence\n",
    "        tokenizer: hanlp tokenizer\n",
    "    输出:\n",
    "        a sentence with words separated by space\n",
    "    '''\n",
    "    words = tokenizer(astr)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def prepare_corpus(tokenizer, corpus, save_json_name):\n",
    "    '''\n",
    "    # 对语料库进行分词\n",
    "    输入:\n",
    "        tokenizer: hanlp tokenizer\n",
    "        corpus: str, the path of corpus\n",
    "        save_json_name: str, the path of saving json file\n",
    "    输出: \n",
    "        \n",
    "    '''\n",
    "    with open(save_json_name, 'r', encoding='utf-8') as fp:\n",
    "        wiki_texts = json.load(fp)\n",
    "        wiki_texts_new = []\n",
    "        for line in tqdm(wiki_texts):\n",
    "            wiki_texts_new.append(filter_str(line, tokenizer))\n",
    "        open(corpus, 'w').write('\\n'.join(wiki_texts_new))\n",
    "\n",
    "# 加载hanlp中的分词模型\n",
    "hanlp_tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)\n",
    "# wiki语料\n",
    "wiki_file = './srilm_data_model/wiki_demo/wiki_z.json'\n",
    "# 分词后语料文件\n",
    "wiki_file_tkd = './srilm_data_model/wiki_demo/wiki_z_word.txt'\n",
    "# 执行\n",
    "prepare_corpus(hanlp_tok, wiki_file_tkd, wiki_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 基于语料库统计的N-gram计算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 从语料库中生成N-gram模型\n",
    "* 将语料库（corpus）和指定的模型设置（ngram）输入模型，在模型存储路径（model_path）中输出统计好的模型\n",
    "* 现成的N-gram语料库：[google n-gram](http://storage.googleapis.com/books/ngrams/books/datasetsv2.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warning: discount coeff 1 is out of range: 0\n",
      "warning: discount coeff 7 is out of range: 1.91919\n"
     ]
    }
   ],
   "source": [
    "def generate_model(model_path, ngram, corpus):\n",
    "    '''\n",
    "    输入:\n",
    "        model_path: str, ngram模型的保存路径\n",
    "        ngram: str, ngram-count路径\n",
    "        corpus: str, corpus路径\n",
    "    输出:\n",
    "        \n",
    "    '''\n",
    "    cmd = '{} -text {} -order 3 -kndiscount3 -lm {}'.format(ngram, corpus, model_path)\n",
    "    os.system(cmd)\n",
    "\n",
    "ngram = '/home/zhang/acoustic_theory/workspace/21-12-30-srilm/srilm/bin/i686-m64/ngram-count'\n",
    "wiki_file_tkd = './srilm_data_model/wiki_demo/wiki_z_word.txt'\n",
    "model_path = './srilm_data_model/wiki_demo/wiki_z_word.lm'\n",
    "generate_model(model_path, ngram, wiki_file_tkd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './srilm_data_model/wiki/wiki_z_word.lm'\n",
    "lm = LM(model_path, lower=True) # 加载N-gram模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 采用N-gram模型计算词频\n",
    "用srilm的LM来调用刚刚生成的模型，采用`lm.logprob_strings(word, context)`来生成 $\\log{p \\left( \\rm{word} | context \\right)}$，word是当前单词，当context是空列表`[]`时相当于1-gram即词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** 计算词频 ********************\n",
      "====================P(的) vs P(西瓜) vs P(桌子)====================\n",
      "P(的): -1.3277089595794678\n",
      "P(西瓜): -5.5793938636779785\n",
      "P(桌子): -5.5162577629089355\n"
     ]
    }
   ],
   "source": [
    "# 计算词频\n",
    "print('*'*20 + ' 计算词频 ' + '*'*20)\n",
    "word_freq0_ = lm.logprob_strings('的', [])\n",
    "word_freq1_ = lm.logprob_strings('西瓜', [])\n",
    "word_freq2_ = lm.logprob_strings('桌子', [])\n",
    "\n",
    "# 输出结果\n",
    "print('='*20 + 'P(的) vs P(西瓜) vs P(桌子)' + '='*20)\n",
    "print('P(的): ' + str(word_freq0_))\n",
    "print('P(西瓜): ' + str(word_freq1_))\n",
    "print('P(桌子): ' + str(word_freq2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 采用N-gram模型计算转移概率\n",
    "\n",
    "当$n>1$时，在`context`中放入前$n-1$个词，顺序是从右到左。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========P(西瓜 | 吃, 喜欢) vs P(桌子 | 吃, 喜欢)==========\n",
      "P(西瓜 | 吃, 喜欢): -2.884925365447998\n",
      "P(桌子 | 吃, 喜欢): -6.211382865905762\n"
     ]
    }
   ],
   "source": [
    "tp1_ = lm.logprob_strings('西瓜', ['吃', '喜欢'])\n",
    "tp2_ = lm.logprob_strings('桌子', ['吃', '喜欢'])\n",
    "print('='*10 + 'P(西瓜 | 吃, 喜欢) vs P(桌子 | 吃, 喜欢)' + '='*10)\n",
    "print('P(西瓜 | 吃, 喜欢): ' + str(tp1_))\n",
    "print('P(桌子 | 吃, 喜欢): ' + str(tp2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 采用N-gram模型计算surprisal\n",
    "$\\rm{surprisal} = -\\log{ \\it{p} \\left( \\rm{word} | context \\right)}$，所以只要取负即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========surprisal(西瓜 | 吃, 喜欢) vs surprisal(桌子 | 吃, 喜欢)==========\n",
      "surprisal(西瓜 | 吃, 喜欢): 2.884925365447998\n",
      "surprisal(桌子 | 吃, 喜欢): 6.211382865905762\n"
     ]
    }
   ],
   "source": [
    "s1_ = -lm.logprob_strings('西瓜', ['吃', '喜欢'])\n",
    "s2_ = -lm.logprob_strings('桌子', ['吃', '喜欢'])\n",
    "print('='*10 + 'surprisal(西瓜 | 吃, 喜欢) vs surprisal(桌子 | 吃, 喜欢)' + '='*10)\n",
    "print('surprisal(西瓜 | 吃, 喜欢): ' + str(s1_))\n",
    "print('surprisal(桌子 | 吃, 喜欢): ' + str(s2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 采用N-gram模型计算entropy\n",
    "$\\rm{entropy} = \\sum \\left( p*surprisal \\right)$，所以对于给定的context，对所有的词来计算surprisal然后求期望"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========entropy(蝴) vs entropy(。)==========\n",
      "entropy(蝴): 0.03182660213747036\n",
      "entropy(。): 2.5136258206385347\n"
     ]
    }
   ],
   "source": [
    "model_path = './srilm_data_model/wiki/wiki_z_morpheme.lm'\n",
    "lm = LM(model_path, lower=True) # 加载N-gram模型\n",
    "def entropy_cal(lm, context):\n",
    "    # entropy\n",
    "    raw_text_idx = [lm.vocab.intern(w) for w in context]\n",
    "    vocab_num = lm.vocab.max_interned() + 1\n",
    "    logprobs = [lm.logprob(i, raw_text_idx) for i in range(vocab_num)]\n",
    "    logprobs_np = np.array(logprobs)\n",
    "    logprobs_np_ = logprobs_np[logprobs_np > -np.inf]\n",
    "    entropy_ = sum(-np.power(10, logprobs_np_)*logprobs_np_)\n",
    "    return entropy_\n",
    "\n",
    "print('='*10 + 'entropy(蝴) vs entropy(。)' + '='*10)\n",
    "e1_ = entropy_cal(lm, ['蝴'])\n",
    "print('entropy(蝴): ' + str(e1_))\n",
    "e2_ = entropy_cal(lm, ['。'])\n",
    "print('entropy(。): ' + str(e2_))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 基于深度学习模型的转移概率计算\n",
    "\n",
    "以gpt-2为例，采用的模型为[gpt2-chinese-cluecorpussmall](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 加载模型，包括分词模型与语言模型\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel\n",
    "ckpt_path = \"uer/gpt2-chinese-cluecorpussmall\" # checkpoint模型路径\n",
    "tokenizer = BertTokenizer.from_pretrained(ckpt_path) # 分词器\n",
    "model = GPT2LMHeadModel.from_pretrained(ckpt_path) # 语言模型"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 获取模型的转移概率\n",
    "后续的surprisal和entropy也可以通过转移概率算出来，与1.2部分类似"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========输入字数: ==========\n",
      "25\n",
      "==========转移概率维度: ==========\n",
      "torch.Size([25, 21128])  输入字数 x 总字数\n"
     ]
    }
   ],
   "source": [
    "model.config.output_hidden_states = True  # 在模型设置config中设置为True，可以让模型输出hidden states\n",
    "inputs = tokenizer('小明喜欢吃西瓜。小明喜欢打篮球。小明经常去花店', return_tensors=\"pt\") # 对句子进行分词\n",
    "outputs = model(**inputs)  # 将分词后的句子输入模型，得到模型输出的结果\n",
    "\n",
    "print('='*10 + '输入字数: ' + '='*10)\n",
    "print(len(inputs['input_ids'][0]))\n",
    "\n",
    "print('='*10 + '转移概率维度: ' + '='*10)\n",
    "print(str(outputs.logits[0].shape) + '  输入字数 x 总字数')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 词性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building model \u001b[5m\u001b[33m...\u001b[0m\u001b[0m          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. 分词结果：\n",
      "['这个', '门', '被', '锁', '了', '，', '锁', '很难', '被', '打开', '。']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 词性标注：\n",
      "['DT', 'NN', 'SB', 'VV', 'SP', 'PU', 'VV', 'AD', 'SB', 'VV', 'PU']\n"
     ]
    }
   ],
   "source": [
    "## 0. 分词\n",
    "sent_ex = '这个门被锁了，锁很难被打开。'\n",
    "tok = hanlp.load(hanlp.pretrained.tok.COARSE_ELECTRA_SMALL_ZH)\n",
    "tks = tok(sent_ex)\n",
    "print('0. 分词结果：')\n",
    "print(tks)\n",
    "\n",
    "## 1. 词性标注\n",
    "pos = hanlp.load(hanlp.pretrained.pos.CTB9_POS_ELECTRA_SMALL)\n",
    "print('1. 词性标注：')\n",
    "print(pos(tks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 词向量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.1 获取静态词向量：以word2vec为例\n",
    "* hanlp支持调用各种静态词向量， 包括[word2vec](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/word2vec.html), [glove](https://hanlp.hankcs.com/docs/api/hanlp/pretrained/glove.html)等等，具体的模型及文献可以在链接文档中进行选择，一般情况下维度越高越准确。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0214,  0.1148, -0.0222,  0.3637, -0.0201,  0.0456,  0.1961,  0.0112,\n",
       "        -0.1819,  0.0498, -0.1650,  0.0608, -0.0347,  0.0285, -0.0964,  0.1015,\n",
       "        -0.0671, -0.1982,  0.1251,  0.1947,  0.0101, -0.0025, -0.1631, -0.2941,\n",
       "        -0.0091, -0.1367,  0.2262,  0.0830, -0.3025, -0.1190, -0.0194,  0.1238,\n",
       "        -0.0772, -0.0300,  0.0903, -0.1235, -0.1933, -0.1664,  0.1258,  0.0885,\n",
       "         0.1188, -0.0032, -0.1931, -0.1691,  0.0525, -0.2215,  0.0959,  0.0981,\n",
       "        -0.1568,  0.1466], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = hanlp.load(hanlp.pretrained.word2vec.CONVSEG_W2V_NEWS_TENSITE_WORD_PKU) # 加载word2vec词向量\n",
    "word2vec('中国')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 捕获了首都信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6510, device='cuda:0')\n",
      "tensor(0.3762, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(torch.nn.functional.cosine_similarity(word2vec('中国')-word2vec('北京'), word2vec('日本')-word2vec('东京'), dim=0))\n",
    "print(torch.nn.functional.cosine_similarity(word2vec('中国')-word2vec('北京'), word2vec('韩国')-word2vec('东京'), dim=0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算相似词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building Trie-based tokenizer for Doc2Vec \u001b[5m\u001b[33m...\u001b[0m\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'上海': 0.8542778491973877, '天津': 0.8106077909469604, '西安': 0.8062387108802795, '哈尔滨': 0.8049991130828857, '武汉': 0.7871935367584229, '南京': 0.7775008678436279, '成都': 0.7753705978393555, '昆明': 0.7722939252853394, '广州': 0.7689048647880554, '兰州': 0.7528482675552368}\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'寒冷': 0.7510591745376587, '非常': 0.7510591745376587, '很': 0.7312489748001099, '比较': 0.6991080045700073, '无比': 0.6859678030014038, '极其': 0.6834491491317749, '十分': 0.6786674857139587, '潮湿': 0.67008376121521, '焦躁不安': 0.6699174642562866, '阴冷': 0.6695234775543213}\n"
     ]
    }
   ],
   "source": [
    "# 单个词\n",
    "print(word2vec.most_similar('北京')) \n",
    "print('\\n')\n",
    "# 当输入不止一个词的时候，设置doc2vec=True，模型会平均所有的词向量，再计算的词\n",
    "print(word2vec.most_similar('非常寒冷', doc2vec=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5.2 获取基于上下文的词向量：语言模型的隐藏层表征\n",
    "\n",
    "同样以1.3中调用的[gpt2-chinese-cluecorpussmall](https://huggingface.co/uer/gpt2-chinese-cluecorpussmall)为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, GPT2LMHeadModel, TextGenerationPipeline\n",
    "ckpt_path = \"uer/gpt2-chinese-cluecorpussmall\" # checkpoint模型路径\n",
    "tokenizer = BertTokenizer.from_pretrained(ckpt_path) # 分词器\n",
    "model = GPT2LMHeadModel.from_pretrained(ckpt_path) # 语言模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========最后一层输出的内隐表征维度: ==========\n",
      "torch.Size([1, 25, 768])  1 x 输入字数 x 表征维度\n"
     ]
    }
   ],
   "source": [
    "model.config.output_hidden_states = True\n",
    "inputs = tokenizer('小明喜欢吃西瓜。小明喜欢打篮球。小明经常去花店', return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "print('\\n' + '='*10 + '最后一层输出的内隐表征维度: ' + '='*10)\n",
    "print(str(outputs.hidden_states[-1].shape) + '  1 x 输入字数 x 表征维度')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 句法特征提取"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 句法特征抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hanlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Dep&nbsp;Tre&nbsp;<br>───────&nbsp;<br>┌┬──┬──&nbsp;<br>││&nbsp;&nbsp;└─►&nbsp;<br>│└─►┌──&nbsp;<br>│&nbsp;&nbsp;&nbsp;└─►&nbsp;<br>└─────►&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Relat&nbsp;<br>─────&nbsp;<br>root&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>dep&nbsp;&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>punct&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">P&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;<br>───────────────────────────────────────<br>_──────────────────────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>_───────────────────►NP────┤&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>_──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►VP&nbsp;───┐&nbsp;&nbsp;&nbsp;<br>_───►NP&nbsp;───┴►VP&nbsp;────►IP&nbsp;───┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►IP<br>_──────────────────────────────────┘&nbsp;&nbsp;&nbsp;</pre></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## workshop中的例子，研究中一般会把标点去掉，但是这里保留了标点，模型也是能够解析标点的\n",
    "Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH) # 选择使用的模型\n",
    "doc = Hanlp('欢迎大家参加工作坊！', tasks=['dep', 'con']) # 在tasks中选择需要的任务，如果不设置就进行所有任务（运行起来会慢一点）\n",
    "doc.pretty_print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 成分句法\n",
    "\n",
    "成分句法输出得到的是一个树结构的数据，可以看作一个嵌套的列表。我们可以：\n",
    "* 访问句法树的一些属性\n",
    "* 转换为括号表示法，计算括号数量\n",
    "* 访问句法树的子树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    }
   ],
   "source": [
    "Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n",
    "doc = Hanlp('欢迎大家参加工作坊！')\n",
    "tree = doc['con']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0)\n",
      "(0, 0, 1, 0, 0)\n",
      "(0, 0, 2, 0, 0, 0)\n",
      "(0, 0, 2, 0, 1, 0, 0)\n",
      "(0, 1, 0)\n"
     ]
    }
   ],
   "source": [
    "# 叶结点的位置\n",
    "for i in range(len(tree.leaves())):\n",
    "    print(tree.leaf_treeposition(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大家'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree[0, 0, 1, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(TOP(IP(VP(VV欢迎)(NP(PN大家))(IP(VP(VV参加)(NP(NN工作坊)))))(PU！)))'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 转为括号表示法\n",
    "bracket_form = tree.pformat().replace ('\\n', '').replace(' ', '') # 去掉换行和空格\n",
    "bracket_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP(IP(VP(VV欢迎)(VP|<NP-IP>(NP(PN大家))(IP(VP(VV参加)(NP(NN工作坊))))))(PU！)))\n"
     ]
    }
   ],
   "source": [
    "# 转换为Chomsky Normal Form，可以用tree.un_chomsky_normal_form()转换回来\n",
    "tree.chomsky_normal_form() \n",
    "bracket_form = tree.pformat().replace ('\\n', '').replace(' ', '')\n",
    "print(bracket_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(TOP|IP(VP(VV欢迎)(VP|<NP-IP>(NP(PN大家))(IP|VP(VV参加)(NP(NN工作坊)))))(PU！))'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出中有些节点只派生出一支，是冗余的（例如最外层的TOP根结点只派生出IP，以及句子中的IP只派生出VP），可以选择压缩节点\n",
    "tree.collapse_unary(collapseRoot=True, joinChar='|') # 压缩冗余节点，压缩的节点用｜来表示\n",
    "bracket_form = tree.pformat().replace ('\\n', '').replace(' ', '')\n",
    "bracket_form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((()((())(()(()))))())\n",
      "左括号数: [3, 3, 2, 2, 1]\n",
      "右括号数: [1, 2, 1, 5, 2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>left_bracket</th>\n",
       "      <th>right_bracket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>欢迎</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>大家</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>参加</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>工作坊</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>！</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word left_bracket right_bracket\n",
       "0   欢迎            3             1\n",
       "1   大家            3             2\n",
       "2   参加            2             1\n",
       "3  工作坊            2             5\n",
       "4    ！            1             2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "# 计算括号表示法中每个词的括号数\n",
    "bracket_clean= re.sub(\"([^()])\", \"\", bracket_form) # 只保留括号\n",
    "print(bracket_clean)\n",
    "\n",
    "# 计算左括号数\n",
    "left_bracket = [len(re.findall(\"\\(\", i)) for i in bracket_clean] \n",
    "left_bracket_count = []\n",
    "for i in left_bracket:\n",
    "    if len(left_bracket_count) == 0 or (i == 1 and j != 1):\n",
    "        left_bracket_count.append(1)\n",
    "    elif i == 1 and j == 1:\n",
    "        left_bracket_count[-1] += 1\n",
    "    j = i\n",
    "print(\"左括号数:\", left_bracket_count)\n",
    "\n",
    "# 计算右括号数\n",
    "right_bracket = [len(re.findall(\"\\)\", i)) for i in bracket_clean] \n",
    "right_bracket_count = []; j = 0\n",
    "for i in right_bracket:\n",
    "    if i == 1 and j != 1:\n",
    "        right_bracket_count.append(1)\n",
    "    elif i == 1 and j == 1:\n",
    "        right_bracket_count[-1] += 1\n",
    "    j = i\n",
    "print(\"右括号数:\", right_bracket_count)\n",
    "\n",
    "# 可以保存为 dataframe 进行进一步的句法特征分析\n",
    "df_bracket = pd.DataFrame([tree.leaves(), left_bracket_count, right_bracket_count]).T\n",
    "df_bracket.columns = ['word', 'left_bracket', 'right_bracket']\n",
    "# df_bracket.to_csv('bracket.csv', index=False) # 保存为csv文件\n",
    "df_bracket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminal nodes: ['欢迎', '大家', '参加', '工作坊', '！']\n",
      "Tree depth: 7\n",
      "Tree productions: [TOP|IP -> VP PU, VP -> VV VP|<NP-IP>, VV -> '欢迎', VP|<NP-IP> -> NP IP|VP, NP -> PN, PN -> '大家', IP|VP -> VV NP, VV -> '参加', NP -> NN, NN -> '工作坊', PU -> '！']\n",
      "Part of Speech: [('欢迎', 'VV'), ('大家', 'PN'), ('参加', 'VV'), ('工作坊', 'NN'), ('！', 'PU')]\n"
     ]
    }
   ],
   "source": [
    "# 句法树的属性\n",
    "print(\"Terminal nodes:\", tree.leaves())\n",
    "print(\"Tree depth:\", tree.height())\n",
    "print(\"Tree productions:\", tree.productions())\n",
    "print(\"Part of Speech:\", tree.pos())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP|IP\n",
      "  (VP\n",
      "    (VV 欢迎)\n",
      "    (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "  (PU ！))\n",
      "(VP (VV 欢迎) (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "(VV 欢迎)\n",
      "(VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊))))\n",
      "(NP (PN 大家))\n",
      "(PN 大家)\n",
      "(IP|VP (VV 参加) (NP (NN 工作坊)))\n",
      "(VV 参加)\n",
      "(NP (NN 工作坊))\n",
      "(NN 工作坊)\n",
      "(PU ！)\n"
     ]
    }
   ],
   "source": [
    "# 句法树的嵌套结构\n",
    "for i in tree.subtrees():  # 根据Tree productions，遍历所有的子树，每一棵子树都是一个Tree对象，可以进行之前相同的操作\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(),\n",
       " (0,),\n",
       " (0, 0),\n",
       " (0, 0, 0),\n",
       " (0, 1),\n",
       " (0, 1, 0),\n",
       " (0, 1, 0, 0),\n",
       " (0, 1, 0, 0, 0),\n",
       " (0, 1, 1),\n",
       " (0, 1, 1, 0),\n",
       " (0, 1, 1, 0, 0),\n",
       " (0, 1, 1, 1),\n",
       " (0, 1, 1, 1, 0),\n",
       " (0, 1, 1, 1, 0, 0),\n",
       " (1,),\n",
       " (1, 0)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 通过索引访问句法树的子树\n",
    "treepositions = tree.treepositions() # 所有节点的索引\n",
    "treepositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP|IP\n",
      "  (VP\n",
      "    (VV 欢迎)\n",
      "    (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "  (PU ！))\n",
      "(VP (VV 欢迎) (VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊)))))\n",
      "(VV 欢迎)\n",
      "欢迎\n",
      "(VP|<NP-IP> (NP (PN 大家)) (IP|VP (VV 参加) (NP (NN 工作坊))))\n",
      "(NP (PN 大家))\n",
      "(PN 大家)\n",
      "大家\n",
      "(IP|VP (VV 参加) (NP (NN 工作坊)))\n",
      "(VV 参加)\n",
      "参加\n",
      "(NP (NN 工作坊))\n",
      "(NN 工作坊)\n",
      "工作坊\n",
      "(PU ！)\n",
      "！\n"
     ]
    }
   ],
   "source": [
    "for i in treepositions: # 遍历所有节点\n",
    "    print(tree[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 依存句法\n",
    "* 依存句法的数据结构更加简单，为一个列表`[(head, relation), ... ]`。列表中第$i$个值中包括了它的核心词的位置以及它与核心词之间的依存关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'root'), (1, 'dobj'), (1, 'dep'), (3, 'dobj'), (1, 'punct')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hanlp = hanlp.load(hanlp.pretrained.mtl.CLOSE_TOK_POS_NER_SRL_DEP_SDP_CON_ELECTRA_SMALL_ZH)\n",
    "doc = Hanlp('欢迎大家参加工作坊！')\n",
    "doc['dep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>head</th>\n",
       "      <th>rel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>欢迎</td>\n",
       "      <td>0</td>\n",
       "      <td>root</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>大家</td>\n",
       "      <td>1</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>参加</td>\n",
       "      <td>1</td>\n",
       "      <td>dep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>工作坊</td>\n",
       "      <td>3</td>\n",
       "      <td>dobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>！</td>\n",
       "      <td>1</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word  head    rel\n",
       "0   欢迎     0   root\n",
       "1   大家     1   dobj\n",
       "2   参加     1    dep\n",
       "3  工作坊     3   dobj\n",
       "4    ！     1  punct"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以保存为 dataframe 进行进一步的句法特征分析\n",
    "df_dep = pd.DataFrame(doc['dep'], columns=['head', 'rel'])\n",
    "df_dep['word'] = doc['tok/fine']\n",
    "df_dep = df_dep[['word', 'head', 'rel']]\n",
    "df_dep"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 批量操作\n",
    "\n",
    "只需要将要处理的句子放在list中，一起进行特征抽取即可。这对所有特征都适用，不仅是句法特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Dep&nbsp;Tree&nbsp;<br>────────&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;┌─►&nbsp;<br>┌───►└──&nbsp;<br>│&nbsp;&nbsp;&nbsp;┌──►&nbsp;<br>│&nbsp;&nbsp;&nbsp;│┌─►&nbsp;<br>│┌─►└┴──&nbsp;<br>││┌─►┌──&nbsp;<br>│││&nbsp;&nbsp;└─►&nbsp;<br>└┴┴──┬──&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└─►&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Relati&nbsp;<br>──────&nbsp;<br>nummod&nbsp;<br>nsubj&nbsp;&nbsp;<br>nn&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>nn&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>nsubj&nbsp;&nbsp;<br>prep&nbsp;&nbsp;&nbsp;<br>pobj&nbsp;&nbsp;&nbsp;<br>root&nbsp;&nbsp;&nbsp;<br>punct&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;<br>──&nbsp;<br>NT&nbsp;<br>M&nbsp;&nbsp;<br>NN&nbsp;<br>NN&nbsp;<br>NN&nbsp;<br>P&nbsp;&nbsp;<br>NR&nbsp;<br>VV&nbsp;<br>PU&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">NER&nbsp;Type&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>────────────&nbsp;<br>───►DATE&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>───►LOCATION&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">SRL&nbsp;PA1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>────────────&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┴►ARGM-TMP&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;├►ARG1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┴►ARGM-LOC&nbsp;<br>╟──►PRED&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Toke&nbsp;<br>────&nbsp;<br>2023&nbsp;<br>年&nbsp;&nbsp;&nbsp;&nbsp;<br>心理&nbsp;&nbsp;&nbsp;<br>语言&nbsp;&nbsp;&nbsp;<br>学会&nbsp;&nbsp;&nbsp;<br>在&nbsp;&nbsp;&nbsp;&nbsp;<br>广州&nbsp;&nbsp;&nbsp;<br>召开&nbsp;&nbsp;&nbsp;<br>。&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;<br>────────────────────────────────<br>NT──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>M&nbsp;───►CLP&nbsp;──┴►QP&nbsp;───┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>NN──┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►NP&nbsp;───┐&nbsp;&nbsp;&nbsp;<br>NN&nbsp;&nbsp;├────────►NP&nbsp;───┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;<br>NN──┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;<br>P&nbsp;──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►IP<br>NR───►NP&nbsp;───┴►PP&nbsp;───┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;│&nbsp;&nbsp;&nbsp;<br>VV───────────►VP&nbsp;───┴►VP────┤&nbsp;&nbsp;&nbsp;<br>PU──────────────────────────┘&nbsp;&nbsp;&nbsp;</pre></div><br><div style=\"display: table; padding-bottom: 1rem;\"><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Dep&nbsp;Tre&nbsp;<br>───────&nbsp;<br>┌┬──┬──&nbsp;<br>││&nbsp;&nbsp;└─►&nbsp;<br>│└─►┌──&nbsp;<br>│&nbsp;&nbsp;&nbsp;└─►&nbsp;<br>└─────►&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Relat&nbsp;<br>─────&nbsp;<br>root&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>dep&nbsp;&nbsp;&nbsp;<br>dobj&nbsp;&nbsp;<br>punct&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;<br>──&nbsp;<br>VV&nbsp;<br>PN&nbsp;<br>VV&nbsp;<br>NN&nbsp;<br>PU&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">SRL&nbsp;PA1&nbsp;&nbsp;<br>────────&nbsp;<br>╟──►PRED&nbsp;<br>───►ARG1&nbsp;<br>◄─┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>◄─┴►ARG2&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">SRL&nbsp;PA2&nbsp;&nbsp;<br>────────&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>╟──►PRED&nbsp;<br>───►ARG1&nbsp;<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Tok&nbsp;<br>───&nbsp;<br>欢迎&nbsp;&nbsp;<br>大家&nbsp;&nbsp;<br>参加&nbsp;&nbsp;<br>工作坊&nbsp;<br>！&nbsp;&nbsp;&nbsp;</pre><pre style=\"display: table-cell; font-family: SFMono-Regular,Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace; white-space: nowrap; line-height: 128%; padding: 0;\">Po&nbsp;&nbsp;&nbsp;&nbsp;3&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;4&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;5&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;7&nbsp;<br>────────────────────────────────────────<br>VV──────────────────────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>PN───────────────────►NP────┤&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<br>VV──────────┐&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►VP&nbsp;───┐&nbsp;&nbsp;&nbsp;<br>NN───►NP&nbsp;───┴►VP&nbsp;────►IP&nbsp;───┘&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├►IP<br>PU──────────────────────────────────┘&nbsp;&nbsp;&nbsp;</pre></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = ['2023年心理语言学会在广州召开。', '欢迎大家参加工作坊！']\n",
    "docs = Hanlp(sentences)\n",
    "docs.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量为: 2\n",
      "['2023', '年', '心理', '语言', '学会', '在', '广州', '召开', '。']\n",
      "['欢迎', '大家', '参加', '工作坊', '！']\n"
     ]
    }
   ],
   "source": [
    "# 提取出来的特征直接索引即可\n",
    "print(\"句子数量为:\", docs.count_sentences())\n",
    "for i in range(docs.count_sentences()):\n",
    "    print(docs['tok/fine'][i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 语言任务\n",
    "在本小节中，我们以主题分析任务和上下文学习为例，演示语言模型的加载和推理过程。对于其他语言任务，均可在huggingface平台搜索到类似的教程文档以及代码。\n",
    "## 3.1 主题分析任务\n",
    "使用transformers管道pipeline快速实现语言任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 单个句子主题分析计算 ====================\n",
      "\n",
      "Input: 欢迎参加工作坊！\n",
      "Prediction: culture, Score: 0.723\n",
      "\n",
      "\n",
      "\n",
      "==================== 多个句子批量进行主题分析计算 ====================\n",
      "\n",
      "Input: 2023年心理语言学会在广州召开\n",
      "Prediction: culture, Score: 0.969\n",
      "\n",
      "Input: 湖人有意签保罗补强，联手詹姆斯追逐总冠军\n",
      "Prediction: sports, Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = 'model/roberta-base-finetuned-chinanews-chinese'\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 利用pipeline快速进行语言任务\n",
    "text = '欢迎参加工作坊！'\n",
    "text_classification = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "res = text_classification(text)[0]\n",
    "print(\"=\"*20, \"单个句子主题分析计算\", \"=\"*20)\n",
    "print(f\"\\nInput: {text}\\nPrediction: {res['label']}, Score: {res['score']:.3f}\")\n",
    "\n",
    "\n",
    "# pipeline可以实现批量句子的计算\n",
    "text_lst = ['2023年心理语言学会在广州召开', '湖人有意签保罗补强，联手詹姆斯追逐总冠军']\n",
    "res_lst = text_classification(text_lst)\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"多个句子批量进行主题分析计算\", \"=\"*20)\n",
    "for text, res in zip(text_lst, res_lst):\n",
    "    print(f\"\\nInput: {text}\\nPrediction: {res['label']}, Score: {res['score']:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 上下文学习\n",
    "通过在上下文中给定任务描述和示例，通用的文本生成模型可以根据上下文快速学习语言任务。在这里我们不使用pipeline，直接调用模型方法进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "==================== 上下文学习实现文本翻译 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhang/anaconda3/envs/ngram/lib/python3.7/site-packages/transformers/generation/utils.py:1278: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: translate English to German: How old are you?\n",
      "Output: Wie alte sind Sie?\n",
      "\n",
      "\n",
      "\n",
      "==================== 上下文学习实现主题文本生成 ====================\n",
      "Input: Generate sentences with the topic : \n",
      "sports => Lionel Messi and MLS club Inter Miami are discussing possible signing\n",
      "entertainment => \n",
      "\n",
      "Output: a new tv series starring adrian sandler is \n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = \"model/flan-t5-large\"\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"上下文学习实现文本翻译\", \"=\"*20)\n",
    "text = \"translate English to German: How old are you?\"\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 调用模型的generate方法\n",
    "outputs = model.generate(input_ids)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "print(f\"Input: {text}\\nOutput: {decoded_output}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"上下文学习实现主题文本生成\", \"=\"*20)\n",
    "text = '''Generate sentences with the topic : \n",
    "sports => Lionel Messi and MLS club Inter Miami are discussing possible signing\n",
    "entertainment => \n",
    "'''\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 调用模型的generate方法\n",
    "outputs = model.generate(input_ids)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "print(f\"Input: {text}\\nOutput: {decoded_output}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 文本生成超参数\n",
    "在本小节中，我们会分析文本生成中的温度参数、搜索策略参数以及top-p参数对文本生成结果的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Welcome to \n",
      "\n",
      "==================== 贪婪搜索 ====================\n",
      "Iter 0: Welcome to the e-commerce world!\n",
      "Iter 1: Welcome to the e-commerce world!\n",
      "Iter 2: Welcome to the e-commerce world!\n",
      "Iter 3: Welcome to the e-commerce world!\n",
      "Iter 4: Welcome to the e-commerce world!\n",
      "==================== 随机搜索, 温度参数=0.1 ====================\n",
      "Iter 0: Welcome to the world of e-commerce\n",
      "Iter 1: Welcome to the world of e-commerce\n",
      "Iter 2: Welcome to the world of e-commerce\n",
      "Iter 3: Welcome to the official website of the \n",
      "Iter 4: Welcome to the e-commerce world!\n",
      "==================== 随机搜索, 温度参数=1.0 ====================\n",
      "Iter 0: www.beauxcarmen.dk\n",
      "Iter 1: Welcome to our website, which has been created\n",
      "Iter 2: Welcome to the New West! Thank you to\n",
      "Iter 3: To visit Aeschweiler,\n",
      "Iter 4: Our visitors can now use the site's\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = \"model/flan-t5-large\"\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "text = 'Welcome to '\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 其余可修改参数包括top_k, top_p等, 可直接在.generate()方法中调用\n",
    "# ref: https://huggingface.co/blog/how-to-generate\n",
    "print(f'\\nInput: {text}\\n')\n",
    "print(\"=\"*20, \"贪婪搜索\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")\n",
    "    \n",
    "print(\"=\"*20, \"随机搜索, 温度参数=0.1\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, do_sample=True, temperature=0.1, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")\n",
    "    \n",
    "\n",
    "print(\"=\"*20, \"随机搜索, 温度参数=1.0\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, do_sample=True, temperature=1.0, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd65688f949d2ad1e75c78c2f6ddcc07f0a0c35d5c97b06568571046f968236a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
